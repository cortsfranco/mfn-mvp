"""
Ejemplo de uso del pipeline RAG
Muestra c√≥mo procesar documentos y crear un √≠ndice vectorial
"""

import asyncio
import os
from pathlib import Path
from typing import List

from app.core.rag_pipeline import rag_pipeline
from app.utils.logger import get_logger

logger = get_logger(__name__)

async def example_process_single_document():
    """Ejemplo: Procesar un solo documento"""
    try:
        logger.info("üìÑ Ejemplo: Procesando un solo documento")
        
        # Crear un archivo de ejemplo (en producci√≥n usar√≠as un archivo real)
        example_content = """
        La inteligencia artificial (IA) es una rama de la inform√°tica que busca crear 
        sistemas capaces de realizar tareas que normalmente requieren inteligencia humana.
        
        Estas tareas incluyen:
        - Aprendizaje autom√°tico
        - Razonamiento l√≥gico
        - Percepci√≥n visual
        - Resoluci√≥n de problemas complejos
        
        El machine learning es un subconjunto de la IA que se enfoca en el desarrollo de 
        algoritmos que pueden aprender y hacer predicciones bas√°ndose en datos.
        
        Los modelos de deep learning utilizan redes neuronales artificiales para procesar 
        informaci√≥n de manera similar al cerebro humano.
        """
        
        # Crear archivo temporal
        temp_file = "example_document.txt"
        with open(temp_file, "w", encoding="utf-8") as f:
            f.write(example_content)
        
        try:
            # Procesar documento completo
            result = await rag_pipeline.process_and_index_document(temp_file)
            
            if result["success"]:
                logger.info(f"‚úÖ Documento procesado exitosamente:")
                logger.info(f"   - Archivo: {result['file_path']}")
                logger.info(f"   - Documentos procesados: {result['documents_processed']}")
                logger.info(f"   - Tiempo de procesamiento: {result['processing_time_seconds']:.2f}s")
                logger.info(f"   - Tama√±o del archivo: {result['file_size_bytes']} bytes")
                logger.info(f"   - Modelo de embeddings: {result['embedding_model']}")
            else:
                logger.error(f"‚ùå Error procesando documento: {result.get('error', 'Error desconocido')}")
            
            return result
            
        finally:
            # Limpiar archivo temporal
            if os.path.exists(temp_file):
                os.remove(temp_file)
                
    except Exception as e:
        logger.error(f"‚ùå Error en ejemplo de documento √∫nico: {str(e)}")
        raise

async def example_process_multiple_documents():
    """Ejemplo: Procesar m√∫ltiples documentos"""
    try:
        logger.info("üìö Ejemplo: Procesando m√∫ltiples documentos")
        
        # Crear m√∫ltiples archivos de ejemplo
        documents = [
            {
                "name": "ai_basics.txt",
                "content": """
                Conceptos b√°sicos de Inteligencia Artificial:
                
                La IA se puede clasificar en:
                1. IA d√©bil (narrow AI): Dise√±ada para tareas espec√≠ficas
                2. IA fuerte (general AI): Capaz de realizar cualquier tarea intelectual humana
                
                Aplicaciones comunes:
                - Reconocimiento de voz
                - Procesamiento de lenguaje natural
                - Visi√≥n por computadora
                - Sistemas de recomendaci√≥n
                """
            },
            {
                "name": "machine_learning.txt",
                "content": """
                Machine Learning Fundamentals:
                
                Tipos de aprendizaje:
                - Supervisado: Con datos etiquetados
                - No supervisado: Sin datos etiquetados
                - Por refuerzo: Aprendizaje basado en recompensas
                
                Algoritmos populares:
                - Regresi√≥n lineal
                - √Årboles de decisi√≥n
                - Redes neuronales
                - Support Vector Machines
                """
            },
            {
                "name": "deep_learning.txt",
                "content": """
                Deep Learning Overview:
                
                Las redes neuronales profundas consisten en:
                - Capas de entrada
                - Capas ocultas
                - Capa de salida
                
                Arquitecturas populares:
                - CNN (Convolutional Neural Networks)
                - RNN (Recurrent Neural Networks)
                - Transformer
                - GAN (Generative Adversarial Networks)
                """
            }
        ]
        
        # Crear archivos temporales
        temp_files = []
        for doc in documents:
            file_path = doc["name"]
            with open(file_path, "w", encoding="utf-8") as f:
                f.write(doc["content"])
            temp_files.append(file_path)
        
        try:
            results = []
            
            # Procesar cada documento
            for file_path in temp_files:
                logger.info(f"üìÑ Procesando: {file_path}")
                result = await rag_pipeline.process_and_index_document(file_path)
                results.append(result)
                
                if result["success"]:
                    logger.info(f"‚úÖ {file_path}: {result['documents_processed']} chunks procesados")
                else:
                    logger.error(f"‚ùå {file_path}: {result.get('error', 'Error desconocido')}")
            
            # Resumen
            successful = sum(1 for r in results if r["success"])
            total_documents = len(results)
            total_chunks = sum(r.get("documents_processed", 0) for r in results if r["success"])
            
            logger.info(f"üìä Resumen del procesamiento:")
            logger.info(f"   - Documentos exitosos: {successful}/{total_documents}")
            logger.info(f"   - Total de chunks procesados: {total_chunks}")
            
            return results
            
        finally:
            # Limpiar archivos temporales
            for file_path in temp_files:
                if os.path.exists(file_path):
                    os.remove(file_path)
                    
    except Exception as e:
        logger.error(f"‚ùå Error en ejemplo de m√∫ltiples documentos: {str(e)}")
        raise

async def example_step_by_step_processing():
    """Ejemplo: Procesamiento paso a paso"""
    try:
        logger.info("üîß Ejemplo: Procesamiento paso a paso")
        
        # Crear archivo de ejemplo
        example_content = """
        Procesamiento paso a paso de documentos:
        
        Este documento ser√° procesado en etapas separadas para demostrar
        el funcionamiento del pipeline RAG.
        
        Etapas del proceso:
        1. Extracci√≥n de texto
        2. Divisi√≥n en chunks
        3. Generaci√≥n de embeddings
        4. Indexaci√≥n en Azure Search
        """
        
        temp_file = "step_by_step_example.txt"
        with open(temp_file, "w", encoding="utf-8") as f:
            f.write(example_content)
        
        try:
            # Paso 1: Procesar documento (extraer texto, chunking, embeddings)
            logger.info("üîÑ Paso 1: Procesando documento...")
            documents = await rag_pipeline.process_document(temp_file)
            
            logger.info(f"‚úÖ Documento procesado: {len(documents)} chunks creados")
            
            # Mostrar informaci√≥n de los chunks
            for i, doc in enumerate(documents[:3]):  # Mostrar solo los primeros 3
                logger.info(f"   Chunk {i+1}: {doc['chunk_size']} caracteres")
            
            # Paso 2: Agregar al √≠ndice de b√∫squeda
            logger.info("üîÑ Paso 2: Agregando al √≠ndice de b√∫squeda...")
            index_success = await rag_pipeline.add_documents_to_search(documents)
            
            if index_success:
                logger.info(f"‚úÖ Documentos agregados al √≠ndice exitosamente")
            else:
                logger.error(f"‚ùå Error agregando documentos al √≠ndice")
            
            return {
                "documents_processed": len(documents),
                "index_success": index_success
            }
            
        finally:
            # Limpiar archivo temporal
            if os.path.exists(temp_file):
                os.remove(temp_file)
                
    except Exception as e:
        logger.error(f"‚ùå Error en ejemplo paso a paso: {str(e)}")
        raise

async def example_validate_pipeline():
    """Ejemplo: Validar el pipeline"""
    try:
        logger.info("üîç Ejemplo: Validando pipeline RAG")
        
        # Validar pipeline
        is_valid = await rag_pipeline.validate_pipeline()
        
        if is_valid:
            logger.info("‚úÖ Pipeline RAG validado correctamente")
        else:
            logger.error("‚ùå Pipeline RAG tiene problemas")
        
        return is_valid
        
    except Exception as e:
        logger.error(f"‚ùå Error validando pipeline: {str(e)}")
        raise

async def main():
    """Funci√≥n principal que ejecuta todos los ejemplos"""
    try:
        logger.info("üöÄ Iniciando ejemplos del pipeline RAG...")
        
        # 1. Validar pipeline
        logger.info("=" * 50)
        await example_validate_pipeline()
        
        # 2. Procesar documento √∫nico
        logger.info("=" * 50)
        await example_process_single_document()
        
        # 3. Procesar m√∫ltiples documentos
        logger.info("=" * 50)
        await example_process_multiple_documents()
        
        # 4. Procesamiento paso a paso
        logger.info("=" * 50)
        await example_step_by_step_processing()
        
        logger.info("üéâ Todos los ejemplos del pipeline RAG completados exitosamente!")
        
    except Exception as e:
        logger.error(f"‚ùå Error en ejemplos del pipeline: {str(e)}")

if __name__ == "__main__":
    # Ejecutar ejemplos
    asyncio.run(main())
